import numpy as np
import math
import random
import re # For parsing

# Ensure Multi_UAV_env.py is in the same directory or accessible in PYTHONPATH
from Multi_UAV_env import Multi_UAV_env_multi_level_practical_multicheck

class DummyArgs:
    def __init__(self):
        self.episode_limit = 200 # Default, will be adjusted if log is longer


def parse_locations(loc_str):
    # Input: "[(-250.0,400.0), (-30.3,930.3), (-30.3,-130.3)]"
    # Output: np.array([[ -250.  ,  400.  ], [  -30.3 ,  930.3 ], [  -30.3 , -130.3 ]])
    try:
        # Remove outer brackets
        if loc_str.startswith('[') and loc_str.endswith(']'):
            loc_str = loc_str[1:-1]
        
        # Find all tuples like (num,num)
        # Corrected regex to handle spaces and negative numbers robustly
        tuple_matches = re.findall(r'\(\s*(-?\d+\.?\d*)\s*,\s*(-?\d+\.?\d*)\s*\)', loc_str)
        
        locations = []
        for match in tuple_matches:
            locations.append([float(match[0]), float(match[1])])
        
        if not locations and loc_str.strip(): # If regex failed but string is not empty
             # Fallback for simpler parsing if regex fails for some edge cases not covered
             # This is a basic fallback, regex is preferred
             raw_tuples = loc_str.split('), (')
             for rt_idx, rt_str in enumerate(raw_tuples):
                 rt_str = rt_str.replace('(', '').replace(')', '')
                 parts = rt_str.split(',')
                 if len(parts) == 2:
                     locations.append([float(parts[0].strip()), float(parts[1].strip())])
                 else:
                     raise ValueError(f"Cannot parse tuple string: {rt_str}")

        if not locations and loc_str.strip(): # If still no locations parsed and string is not empty
            raise ValueError(f"Could not parse any location tuples from: {loc_str}")
            
        return np.array(locations, dtype=float)
    except Exception as e:
        print(f"Error parsing location string: '{loc_str}' - {e}")
        raise # Re-raise to be caught by the caller

def parse_actions(act_str):
    # Input: "[1. 1. 3.]" or "[1., 1., 3.]"
    # Output: [1, 1, 3] (list of ints)
    try:
        # Remove brackets and split by space or comma, then filter empty strings
        # Handles both space and comma as separators for robustness
        cleaned_act_str = act_str.strip('[]')
        action_values = [val for val in re.split(r'[\s,]+', cleaned_act_str) if val]
        return [int(float(x)) for x in action_values]
    except Exception as e:
        print(f"Error parsing actions string: '{act_str}' - {e}")
        raise

def recalculate_rewards_from_log(log_file_path):
    args = DummyArgs()
    # Initialize with default episode limit, will be updated
    env = Multi_UAV_env_multi_level_practical_multicheck(args)

    log_total_reward_header = None
    log_bs_locations = None
    log_episode_length = None
    
    parsed_steps_data = []

    with open(log_file_path, 'r') as f:
        lines = f.readlines()

    header_info_parsed = False
    data_section_started = False
    
    print("--- Parsing Log File ---")
    for line_idx, line in enumerate(lines):
        line = line.strip()
        if not line:
            continue

        if not header_info_parsed:
            if "Total Reward:" in line:
                try:
                    log_total_reward_header = float(line.split("Total Reward:")[1].split(',')[0].strip())
                    print(f"Parsed Total Reward from header: {log_total_reward_header}")
                except ValueError:
                    print(f"Warning: Could not parse Total Reward from line: {line}")
            
            if "Length:" in line:
                try:
                    log_episode_length = int(line.split("Length:")[1].split(',')[0].strip())
                    print(f"Parsed Episode Length from header: {log_episode_length}")
                    args.episode_limit = max(args.episode_limit, log_episode_length + 5) # Adjust episode limit based on log
                    env.episode_limit = args.episode_limit # Update env instance too
                except ValueError:
                    print(f"Warning: Could not parse Episode Length from line: {line}")


            if "BS Locations:" in line:
                try:
                    bs_loc_str = line.split("BS Locations:")[1].strip()
                    match = re.search(r'\[\s*\[\s*([-+]?\d*\.?\d+)\s*,\s*([-+]?\d*\.?\d+)\s*\]\s*\]', bs_loc_str)
                    if match:
                        log_bs_locations = np.array([[float(match.group(1)), float(match.group(2))]], dtype=float)
                        print(f"Parsed BS Locations: {log_bs_locations}")
                    else:
                         print(f"Warning: Could not parse BS Locations from string: {bs_loc_str} using regex.")
                except Exception as e:
                     print(f"Warning: Error parsing BS Locations from line: {line} due to {e}")
        
        if "Step | Agent Actions | Reward | Terminated" in line:
            data_section_started = True
            header_info_parsed = True # Assume header parsing is complete
            print("Found data section header.")
            continue

        if data_section_started and (line.startswith("---") or not line):
            continue

        if data_section_started:
            parts = [p.strip() for p in line.split('|')]
            if len(parts) == 6:
                try:
                    step_num_str = parts[0]
                    actions_str = parts[1]
                    reward_log_str = parts[2]
                    terminated_log_str = parts[3]
                    locations_str = parts[4]
                    collisions_log_str = parts[5]

                    step_num = int(step_num_str)
                    reward_log_val = float(reward_log_str)
                    terminated_log_val = int(terminated_log_str)
                    collisions_log_val = int(collisions_log_str)
                    
                    parsed_actions = parse_actions(actions_str)
                    parsed_locations = parse_locations(locations_str)
                                        
                    parsed_steps_data.append({
                        "step_num": step_num,
                        "actions": parsed_actions,
                        "reward_log": reward_log_val,
                        "terminated_log": terminated_log_val,
                        "locations_log": parsed_locations,
                        "collisions_log": collisions_log_val
                    })
                except Exception as e: # Catch parsing errors for actions/locations or type conversions
                    print(f"Critical Error: Could not parse data line {line_idx+1}: '{line}' - {e}. Aborting step parsing for this line.")
            elif line:
                print(f"Warning: Skipping unexpected line format in data section: {line}")

    if not parsed_steps_data:
        print("No step data successfully parsed from log.")
        return

    print(f"Successfully parsed {len(parsed_steps_data)} steps from the log.")

    # --- Simulation Part ---
    print("\n--- Starting Recalculation ---")
    env.reset() 
    
    if log_bs_locations is not None:
        print(f"Setting BS Locations from log: {log_bs_locations}")
        env.BS_locations = np.copy(log_bs_locations)
    else:
        print("Warning: BS locations not found in log or failed to parse, using default/randomized from env.reset(). Recalculation might differ significantly.")

    total_recalculated_reward = 0.0
    
    initial_log_locs = parsed_steps_data[0]["locations_log"]
    if not np.allclose(env.current_location, initial_log_locs, atol=1e-1): # Looser tolerance for float comparisons
        print("Warning: Initial locations from env.reset() do not closely match log's first step locations (Step 0).")
        print(f"  Env initial after reset: {env.current_location}")
        print(f"  Log initial (Step 0): {initial_log_locs}")
        print("  Setting env initial locations to log's initial locations for consistency.")
        env.current_location = np.copy(initial_log_locs)
    
    # Ensure other relevant initial states are as expected from a reset
    env.is_arrived[:] = False
    env.recorded_arrive = []
    if hasattr(env, 'all_arrived_bonus_given_flag'): # Ensure it's reset
        delattr(env, 'all_arrived_bonus_given_flag')
    env.episode_step = 0


    print("\nStep | Logged Actions   | Log Reward | Recalc Reward | Term Log | Term Recalc | Coll Log | Coll Recalc | Reward Match?")
    print("-----------------------------------------------------------------------------------------------------------------------")

    for i, step_data in enumerate(parsed_steps_data):
        log_step_num = step_data["step_num"]
        actions = step_data["actions"]
        reward_log = step_data["reward_log"]
        terminated_log = step_data["terminated_log"]
        locations_log_at_start_of_step = step_data["locations_log"]
        collisions_log = step_data["collisions_log"]

        if env.episode_step != log_step_num:
            print(f"State Sync: Log step {log_step_num}, Env episode_step was {env.episode_step}. Syncing env.episode_step.")
            env.episode_step = log_step_num

        if not np.allclose(env.current_location, locations_log_at_start_of_step, atol=1e-1): # Looser tolerance
            print(f"State Sync: Location mismatch at start of log step {log_step_num}:")
            print(f"  Env current_location: {env.current_location.round(2)}")
            print(f"  Log locations:        {locations_log_at_start_of_step.round(2)}")
            print("  Setting env.current_location to log's state for this step.")
            env.current_location = np.copy(locations_log_at_start_of_step)
            # We also need to ensure 'is_arrived' is consistent with these locations if forced.
            # This typically means recalculating 'is_arrived' based on 'env.current_location' and 'env.dest_location'.
            # For simplicity, env.step() will do its own arrival check after movement.
            # A discrepancy here means the previous step's simulation didn't match the log.
            for agent_k in range(env.n_a_agents):
                dist_to_dest = np.linalg.norm(env.current_location[agent_k] - env.dest_location[agent_k])
                if dist_to_dest < 10.0: # Arrival threshold
                    if not env.is_arrived[agent_k]:
                        print(f"  Manually setting agent {agent_k} to arrived due to location override.")
                        env.is_arrived[agent_k] = True
                        # This might affect 'recorded_arrive' logic if not handled carefully.
                        # The 'step' function's reward logic for arrival depends on 'recorded_arrive'.
                        # If an agent is forced to 'arrived', it should ideally be added to 'recorded_arrive'
                        # if the reward hasn't been given yet. This gets complex to perfectly sync.
                else:
                    if env.is_arrived[agent_k]: # Should not be arrived if far
                        print(f"  Manually setting agent {agent_k} to NOT arrived due to location override.")
                        env.is_arrived[agent_k] = False
                        if agent_k in env.recorded_arrive:
                           # This is tricky: if it was marked as arrived for reward, then forced to not be.
                           # For now, we assume the log is the source of truth for locations at step start.
                           pass


        recalc_reward, recalc_terminated, recalc_info = env.step(actions)
        
        total_recalculated_reward += recalc_reward
        recalc_collisions = recalc_info.get('collisions', 0)

        reward_match_str = "MATCH" if abs(reward_log - recalc_reward) < 1e-4 else "MISMATCH" # Tighter tolerance for reward
        
        term_recalc_str = "True " if recalc_terminated else "False" # Ensure fixed width

        print(f"{log_step_num:4d} | {str(actions):<16} | {reward_log:10.2f} | {recalc_reward:13.2f} | {terminated_log:8d} | {term_recalc_str:<11} | {collisions_log:8d} | {recalc_collisions:11d} | {reward_match_str}")

        if bool(terminated_log) != bool(recalc_terminated):
             print(f"  MISMATCH: Termination status. Log: {bool(terminated_log)}, Recalculated: {bool(recalc_terminated)}")
        
        if recalc_terminated:
            print("Recalculated episode terminated at this step.")
            if i < len(parsed_steps_data) - 1 and not terminated_log : # If recalc terminated early
                 print("  Log continued for more steps. Recalculation stopped.")
            break 
        elif terminated_log and i == len(parsed_steps_data) -1 : # Log terminated at its last step
            print("Log showed termination at its last step.")


    print("\n--- Recalculation Summary ---")
    if log_total_reward_header is not None:
        print(f"Total Reward from Log Header:       {log_total_reward_header:.2f}")
    else:
        print("Total Reward from Log Header:       N/A (Not parsed)")
    
    sum_rewards_from_log_lines = sum(s["reward_log"] for s in parsed_steps_data)
    print(f"Sum of Step Rewards from Log Lines: {sum_rewards_from_log_lines:.2f}")
    print(f"Total Recalculated Reward:          {total_recalculated_reward:.2f}")

    if log_total_reward_header is not None:
        if abs(log_total_reward_header - total_recalculated_reward) < 1e-4: # Compare with a small tolerance
            print("\nOverall total reward MATCHES log header.")
        else:
            print("\nOverall total reward MISMATCHES log header.")
            print(f"  Difference (Header - Recalculated): {log_total_reward_header - total_recalculated_reward:.4f}")

    if abs(sum_rewards_from_log_lines - total_recalculated_reward) < 1e-4:
        print("Overall total reward MATCHES sum of log line rewards.")
    else:
        print("Overall total reward MISMATCHES sum of log line rewards.")
        print(f"  Difference (Log Sum - Recalculated): {sum_rewards_from_log_lines - total_recalculated_reward:.4f}")

# ==============================================================================
# == 请在此处填入您的日志文件路径                                               ==
# ==============================================================================
log_file_path = "gemini_2.log"
# Example: log_file_path = "log/20250601-184150/vdn_20250602-035542_eval_epeval_T2000005_ep0.log"

if __name__ == '__main__':
    if log_file_path == "PASTE_YOUR_LOG_FILE_PATH_HERE.log":
        print("Please update the 'log_file_path' variable in the script with the actual path to your log file.")
    else:
        recalculate_rewards_from_log(log_file_path)
